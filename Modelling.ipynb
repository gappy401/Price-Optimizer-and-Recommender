{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1b6d4d",
   "metadata": {},
   "source": [
    "MODEL BUILDING FOR PRICE OPTIMIZATION\n",
    "\n",
    "BUSINESS OBJECTIVE: Build ML model to predict profit at different price points\n",
    "TECHNICAL OBJECTIVE: Compare models, select best for production deployment\n",
    "\n",
    "MODEL SELECTION CRITERIA:\n",
    "1. Prediction accuracy (R¬≤, MAE, RMSE)\n",
    "2. Interpretability (can we explain to stakeholders?)\n",
    "3. Speed (real-time pricing decisions)\n",
    "4. Robustness (works across all products/segments)\n",
    "\n",
    "MODELS TO TEST:\n",
    "1. Linear Regression - Baseline, highly interpretable\n",
    "2. Ridge Regression - Regularized linear, prevents overfitting\n",
    "3. Random Forest - Non-linear, feature importance, robust\n",
    "4. Gradient Boosting - Best accuracy, industry standard\n",
    "5. XGBoost - Production-ready, fast, accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fc1b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. LOAD DATA\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded: 10,000 records with 54 columns\n",
      "‚úì Features: 35\n",
      "‚úì Target: profit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA AND FEATURES\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"1. LOAD DATA\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "df = pd.read_csv('lab_equipment_pricing_features.csv')\n",
    "print(f\"‚úì Loaded: {len(df):,} records with {df.shape[1]} columns\")\n",
    "\n",
    "# Load feature metadata\n",
    "with open('feature_metadata.json', 'r') as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "all_features = feature_metadata['all_features']\n",
    "target = feature_metadata['target']\n",
    "\n",
    "print(f\"‚úì Features: {len(all_features)}\")\n",
    "print(f\"‚úì Target: {target}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc14611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "2. TRAIN/TEST SPLIT STRATEGY\n",
      "================================================================================\n",
      "\n",
      "WHY TIME-BASED SPLIT:\n",
      "  - Can't use random split (data leakage risk)\n",
      "  - Must simulate real scenario: train on past, predict future\n",
      "  - Business: Model must work on upcoming quarters\n",
      "\n",
      "Rows after removing NaN: 10,000\n",
      "\n",
      "Train set: 8,000 records (80%)\n",
      "Test set: 2,000 records (20%)\n",
      "Train target mean: $467,910\n",
      "Test target mean: $452,682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPARE TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"2. TRAIN/TEST SPLIT STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY TIME-BASED SPLIT:\")\n",
    "print(\"  - Can't use random split (data leakage risk)\")\n",
    "print(\"  - Must simulate real scenario: train on past, predict future\")\n",
    "print(\"  - Business: Model must work on upcoming quarters\")\n",
    "print()\n",
    "\n",
    "# Sort by date\n",
    "df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Remove rows with NaN in features (from rolling calculations)\n",
    "df_model = df_sorted[all_features + [target]].dropna()\n",
    "print(f\"Rows after removing NaN: {len(df_model):,}\")\n",
    "\n",
    "X = df_model[all_features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Time-based split: 80% train, 20% test\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} records (80%)\")\n",
    "print(f\"Test set: {len(X_test):,} records (20%)\")\n",
    "print(f\"Train target mean: ${y_train.mean():,.0f}\")\n",
    "print(f\"Test target mean: ${y_test.mean():,.0f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5627456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "3. FEATURE SCALING\n",
      "================================================================================\n",
      "\n",
      "WHY SCALING:\n",
      "  - Linear models sensitive to feature scales\n",
      "  - Tree models don't need scaling (but doesn't hurt)\n",
      "  - Speeds up convergence\n",
      "\n",
      "‚úì Features scaled using StandardScaler\n",
      "  Mean: ~0, Std: ~1 for all features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"3. FEATURE SCALING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY SCALING:\")\n",
    "print(\"  - Linear models sensitive to feature scales\")\n",
    "print(\"  - Tree models don't need scaling (but doesn't hurt)\")\n",
    "print(\"  - Speeds up convergence\")\n",
    "print()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì Features scaled using StandardScaler\")\n",
    "print(f\"  Mean: ~0, Std: ~1 for all features\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e35855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4. MODEL 1: LINEAR REGRESSION (BASELINE)\n",
      "================================================================================\n",
      "\n",
      "WHY LINEAR REGRESSION:\n",
      "  Business: Highly interpretable, shows feature importance\n",
      "  Technical: Fast, simple, good baseline\n",
      "  Limitation: Assumes linear relationships\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.8683\n",
      "  Test R¬≤: 0.8548\n",
      "  Test MAE: $130,994\n",
      "  Test RMSE: $170,992\n",
      "  Test MAPE: 273.5%\n",
      "\n",
      "Top 10 Most Important Features (by coefficient magnitude):\n",
      "                feature    coefficient\n",
      "price_pct_vs_competitor  922799.049438\n",
      " price_ratio_competitor -922754.994543\n",
      "          inventory_pct  609219.710505\n",
      "        inventory_level -607863.198778\n",
      "                  price  187453.749424\n",
      "       competitor_price  184646.196100\n",
      "        segment_encoded   99683.411301\n",
      "            price_ma_7d   89570.647195\n",
      "           price_ma_30d  -62655.987861\n",
      "     is_summer_slowdown  -47234.461899\n",
      "\n",
      "BUSINESS INTERPRETATION:\n",
      "  Most influential: price_pct_vs_competitor (coef: 922799.05)\n",
      "  ‚Üí Each unit increase in price_pct_vs_competitor changes profit by $922799.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: LINEAR REGRESSION (BASELINE)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"4. MODEL 1: LINEAR REGRESSION (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY LINEAR REGRESSION:\")\n",
    "print(\"  Business: Highly interpretable, shows feature importance\")\n",
    "print(\"  Technical: Fast, simple, good baseline\")\n",
    "print(\"  Limitation: Assumes linear relationships\")\n",
    "print()\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_lr = lr_model.predict(X_train_scaled)\n",
    "y_pred_test_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_r2_lr = r2_score(y_train, y_pred_train_lr)\n",
    "test_r2_lr = r2_score(y_test, y_pred_test_lr)\n",
    "test_mae_lr = mean_absolute_error(y_test, y_pred_test_lr)\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_test_lr))\n",
    "test_mape_lr = mean_absolute_percentage_error(y_test, y_pred_test_lr)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_lr:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_lr:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_lr:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_lr:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_lr:.1%}\")\n",
    "print()\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'coefficient': lr_model.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by coefficient magnitude):\")\n",
    "print(feature_importance_lr.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"BUSINESS INTERPRETATION:\")\n",
    "top_feature = feature_importance_lr.iloc[0]\n",
    "print(f\"  Most influential: {top_feature['feature']} (coef: {top_feature['coefficient']:.2f})\")\n",
    "print(f\"  ‚Üí Each unit increase in {top_feature['feature']} changes profit by ${top_feature['coefficient']:.2f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ac5e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "5. MODEL 2: RIDGE REGRESSION (REGULARIZED)\n",
      "================================================================================\n",
      "\n",
      "WHY RIDGE:\n",
      "  Business: Same interpretability as linear, but more robust\n",
      "  Technical: L2 regularization prevents overfitting\n",
      "  Use case: When we have many correlated features\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.8682\n",
      "  Test R¬≤: 0.8549\n",
      "  Test MAE: $130,944\n",
      "  Test RMSE: $170,881\n",
      "  Test MAPE: 273.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: RIDGE REGRESSION (REGULARIZED)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"5. MODEL 2: RIDGE REGRESSION (REGULARIZED)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY RIDGE:\")\n",
    "print(\"  Business: Same interpretability as linear, but more robust\")\n",
    "print(\"  Technical: L2 regularization prevents overfitting\")\n",
    "print(\"  Use case: When we have many correlated features\")\n",
    "print()\n",
    "\n",
    "ridge_model = Ridge(alpha=10.0)  # Regularization strength\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_ridge = ridge_model.predict(X_train_scaled)\n",
    "y_pred_test_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_r2_ridge = r2_score(y_train, y_pred_train_ridge)\n",
    "test_r2_ridge = r2_score(y_test, y_pred_test_ridge)\n",
    "test_mae_ridge = mean_absolute_error(y_test, y_pred_test_ridge)\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_test_ridge))\n",
    "test_mape_ridge = mean_absolute_percentage_error(y_test, y_pred_test_ridge)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_ridge:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_ridge:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_ridge:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_ridge:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_ridge:.1%}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc12d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6. MODEL 3: RANDOM FOREST\n",
      "================================================================================\n",
      "\n",
      "WHY RANDOM FOREST:\n",
      "  Business: Captures non-linear relationships (price curves aren't straight lines)\n",
      "  Technical: Robust, handles outliers well, provides feature importance\n",
      "  Use case: When relationships are complex\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.9580\n",
      "  Test R¬≤: 0.9385\n",
      "  Test MAE: $64,946\n",
      "  Test RMSE: $111,266\n",
      "  Test MAPE: 17.2%\n",
      "\n",
      "Top 10 Most Important Features (by RF importance):\n",
      "              feature  importance\n",
      "                price    0.219923\n",
      "         price_ma_30d    0.178998\n",
      "     competitor_price    0.162480\n",
      "          price_ma_7d    0.125968\n",
      "      product_encoded    0.100218\n",
      "      segment_encoded    0.070031\n",
      "price_diff_competitor    0.040215\n",
      "            qty_ma_7d    0.025073\n",
      "           qty_ma_30d    0.012561\n",
      "   is_summer_slowdown    0.008494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: RANDOM FOREST (NON-LINEAR)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"6. MODEL 3: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY RANDOM FOREST:\")\n",
    "print(\"  Business: Captures non-linear relationships (price curves aren't straight lines)\")\n",
    "print(\"  Technical: Robust, handles outliers well, provides feature importance\")\n",
    "print(\"  Use case: When relationships are complex\")\n",
    "print()\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,        # Number of trees\n",
    "    max_depth=15,            # Prevent overfitting\n",
    "    min_samples_split=20,    # Minimum samples to split\n",
    "    min_samples_leaf=10,     # Minimum samples per leaf\n",
    "    max_features='sqrt',     # Features per split\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)  # Trees don't need scaling\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2_rf = r2_score(y_train, y_pred_train_rf)\n",
    "test_r2_rf = r2_score(y_test, y_pred_test_rf)\n",
    "test_mae_rf = mean_absolute_error(y_test, y_pred_test_rf)\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
    "test_mape_rf = mean_absolute_percentage_error(y_test, y_pred_test_rf)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_rf:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_rf:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_rf:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_rf:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_rf:.1%}\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by RF importance):\")\n",
    "print(feature_importance_rf.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc642950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "7. MODEL 4: GRADIENT BOOSTING\n",
      "================================================================================\n",
      "\n",
      "WHY GRADIENT BOOSTING:\n",
      "  Business: Industry standard for pricing, high accuracy\n",
      "  Technical: Sequential learning, corrects previous errors\n",
      "  Use case: When we need best possible predictions\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.9779\n",
      "  Test R¬≤: 0.9670\n",
      "  Test MAE: $51,315\n",
      "  Test RMSE: $81,482\n",
      "  Test MAPE: 18.6%\n",
      "\n",
      "Top 10 Most Important Features (by GB importance):\n",
      "              feature  importance\n",
      "                price    0.228834\n",
      "         price_ma_30d    0.202077\n",
      "      product_encoded    0.147952\n",
      "      segment_encoded    0.110623\n",
      "          price_ma_7d    0.097988\n",
      "     competitor_price    0.097488\n",
      "price_diff_competitor    0.025583\n",
      "            qty_ma_7d    0.019619\n",
      "           qty_ma_30d    0.015748\n",
      "    is_academic_start    0.010357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 4: GRADIENT BOOSTING (BEST PERFORMANCE)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"7. MODEL 4: GRADIENT BOOSTING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY GRADIENT BOOSTING:\")\n",
    "print(\"  Business: Industry standard for pricing, high accuracy\")\n",
    "print(\"  Technical: Sequential learning, corrects previous errors\")\n",
    "print(\"  Use case: When we need best possible predictions\")\n",
    "print()\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,        # More trees = better but slower\n",
    "    learning_rate=0.05,      # Smaller = more conservative\n",
    "    max_depth=5,             # Tree depth\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    subsample=0.8,           # Use 80% of data per tree\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_gb = gb_model.predict(X_train)\n",
    "y_pred_test_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2_gb = r2_score(y_train, y_pred_train_gb)\n",
    "test_r2_gb = r2_score(y_test, y_pred_test_gb)\n",
    "test_mae_gb = mean_absolute_error(y_test, y_pred_test_gb)\n",
    "test_rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_test_gb))\n",
    "test_mape_gb = mean_absolute_percentage_error(y_test, y_pred_test_gb)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_gb:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_gb:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_gb:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_gb:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_gb:.1%}\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by GB importance):\")\n",
    "print(feature_importance_gb.head(10).to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1c4147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "8. MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "            Model  Train_R¬≤  Test_R¬≤    Test_MAE   Test_RMSE  Test_MAPE_%\n",
      "Linear Regression    0.8683   0.8548 130993.9822 170991.5566     273.5090\n",
      " Ridge Regression    0.8682   0.8549 130943.6738 170881.4504     272.9689\n",
      "    Random Forest    0.9580   0.9385  64945.8018 111265.8225      17.1546\n",
      "Gradient Boosting    0.9779   0.9670  51315.0579  81482.2924      18.6169\n",
      "\n",
      "üèÜ BEST MODEL: Gradient Boosting\n",
      "   Test R¬≤: 0.9670\n",
      "   Explains 96.7% of profit variation\n",
      "\n",
      "BUSINESS INTERPRETATION:\n",
      "  ‚úì Gradient Boosting wins - complex non-linear relationships captured\n",
      "  ‚úì Trade-off: Less interpretable than linear models\n",
      "  ‚úì Solution: Use SHAP values or feature importance for explainability\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"8. MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Train_R¬≤': [train_r2_lr, train_r2_ridge, train_r2_rf, train_r2_gb],\n",
    "    'Test_R¬≤': [test_r2_lr, test_r2_ridge, test_r2_rf, test_r2_gb],\n",
    "    'Test_MAE': [test_mae_lr, test_mae_ridge, test_mae_rf, test_mae_gb],\n",
    "    'Test_RMSE': [test_rmse_lr, test_rmse_ridge, test_rmse_rf, test_rmse_gb],\n",
    "    'Test_MAPE_%': [test_mape_lr*100, test_mape_ridge*100, test_mape_rf*100, test_mape_gb*100]\n",
    "})\n",
    "\n",
    "comparison = comparison.round(4)\n",
    "print(comparison.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Select best model\n",
    "best_idx = comparison['Test_R¬≤'].idxmax()\n",
    "best_model_name = comparison.loc[best_idx, 'Model']\n",
    "best_model_r2 = comparison.loc[best_idx, 'Test_R¬≤']\n",
    "\n",
    "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {best_model_r2:.4f}\")\n",
    "print(f\"   Explains {best_model_r2*100:.1f}% of profit variation\")\n",
    "print()\n",
    "\n",
    "# Business interpretation\n",
    "print(\"BUSINESS INTERPRETATION:\")\n",
    "if best_model_name == 'Gradient Boosting':\n",
    "    print(\"  ‚úì Gradient Boosting wins - complex non-linear relationships captured\")\n",
    "    print(\"  ‚úì Trade-off: Less interpretable than linear models\")\n",
    "    print(\"  ‚úì Solution: Use SHAP values or feature importance for explainability\")\n",
    "    best_model = gb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    print(\"  ‚úì Random Forest wins - good balance of accuracy and speed\")\n",
    "    print(\"  ‚úì Feature importance built-in for explainability\")\n",
    "    best_model = rf_model\n",
    "else:\n",
    "    print(\"  ‚úì Linear model wins - simple relationships, highly interpretable\")\n",
    "    print(\"  ‚úì Can directly explain coefficient impact\")\n",
    "    best_model = ridge_model if best_model_name == 'Ridge Regression' else lr_model\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69d56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "9. CROSS-VALIDATION (TIME SERIES)\n",
      "================================================================================\n",
      "\n",
      "WHY CROSS-VALIDATION:\n",
      "  - Validate model stability across different time periods\n",
      "  - Detect overfitting\n",
      "  - Ensure model works on future data\n",
      "\n",
      "Running 5-fold time series cross-validation on best model...\n",
      "\n",
      "CV R¬≤ Scores: ['0.8637', '0.9510', '0.9635', '0.9605', '0.9662']\n",
      "Mean CV R¬≤: 0.9410 (+/- 0.0390)\n",
      "\n",
      "‚úì Low variance across folds - model is stable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"9. CROSS-VALIDATION (TIME SERIES)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY CROSS-VALIDATION:\")\n",
    "print(\"  - Validate model stability across different time periods\")\n",
    "print(\"  - Detect overfitting\")\n",
    "print(\"  - Ensure model works on future data\")\n",
    "print()\n",
    "\n",
    "# Time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Running 5-fold time series cross-validation on best model...\")\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=tscv, \n",
    "                             scoring='r2', n_jobs=-1)\n",
    "\n",
    "print(f\"\\nCV R¬≤ Scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "print(f\"Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "print()\n",
    "\n",
    "if cv_scores.std() < 0.05:\n",
    "    print(\"‚úì Low variance across folds - model is stable\")\n",
    "else:\n",
    "    print(\"‚ö† High variance - model performance varies by time period\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83efc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "10. RESIDUAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "WHY RESIDUALS:\n",
      "  - Check for systematic errors (bias)\n",
      "  - Validate model assumptions\n",
      "  - Identify segments where model struggles\n",
      "\n",
      "Residual Statistics:\n",
      "  Mean error: $-1,029 (should be ~0)\n",
      "  Median error: $-718\n",
      "  Std dev: $81,496\n",
      "\n",
      "Mean Absolute Error by Product:\n",
      "product\n",
      "Microscope     $103,226\n",
      "Centrifuge      $89,649\n",
      "PCR_System      $55,657\n",
      "Pipettes         $6,515\n",
      "Reagent_Kit      $6,185\n",
      "Name: abs_error, dtype: object\n",
      "\n",
      "BUSINESS INSIGHT:\n",
      "  Hardest to predict: Microscope (error: $103,226)\n",
      "  Easiest to predict: Reagent_Kit (error: $6,185)\n",
      "  ‚Üí May need product-specific models or more features for Microscope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RESIDUAL ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"10. RESIDUAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY RESIDUALS:\")\n",
    "print(\"  - Check for systematic errors (bias)\")\n",
    "print(\"  - Validate model assumptions\")\n",
    "print(\"  - Identify segments where model struggles\")\n",
    "print()\n",
    "\n",
    "residuals = y_test - y_pred_test_gb\n",
    "\n",
    "print(\"Residual Statistics:\")\n",
    "print(f\"  Mean error: ${residuals.mean():,.0f} (should be ~0)\")\n",
    "print(f\"  Median error: ${residuals.median():,.0f}\")\n",
    "print(f\"  Std dev: ${residuals.std():,.0f}\")\n",
    "print()\n",
    "\n",
    "# Check for bias by segment\n",
    "df_test = df_sorted[split_idx:split_idx+len(y_test)].copy()\n",
    "df_test['residual'] = residuals.values\n",
    "df_test['abs_error'] = np.abs(residuals.values)\n",
    "\n",
    "print(\"Mean Absolute Error by Product:\")\n",
    "product_errors = df_test.groupby('product')['abs_error'].mean().sort_values(ascending=False)\n",
    "print(product_errors.apply(lambda x: f\"${x:,.0f}\"))\n",
    "print()\n",
    "\n",
    "print(\"BUSINESS INSIGHT:\")\n",
    "worst_product = product_errors.index[0]\n",
    "best_product = product_errors.index[-1]\n",
    "print(f\"  Hardest to predict: {worst_product} (error: ${product_errors.iloc[0]:,.0f})\")\n",
    "print(f\"  Easiest to predict: {best_product} (error: ${product_errors.iloc[-1]:,.0f})\")\n",
    "print(f\"  ‚Üí May need product-specific models or more features for {worst_product}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "096dfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "11. SAVE FINAL MODEL FOR PRODUCTION\n",
      "================================================================================\n",
      "\n",
      "‚úì Saved: price_optimization_model.pkl (Gradient Boosting)\n",
      "‚úì Saved: feature_scaler.pkl\n",
      "‚úì Saved: model_metadata.json\n",
      "\n",
      "================================================================================\n",
      "MODEL BUILDING COMPLETE - PRODUCTION READY\n",
      "================================================================================\n",
      "\n",
      "DELIVERABLES:\n",
      "  ‚úì Trained model: price_optimization_model.pkl\n",
      "  ‚úì Feature scaler: feature_scaler.pkl\n",
      "  ‚úì Model metadata: model_metadata.json\n",
      "  ‚úì Feature definitions: feature_metadata.json\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "  Test R¬≤: 0.9670 (96.7% variance explained)\n",
      "  Test MAE: $51,315\n",
      "  Test MAPE: 18.6%\n",
      "\n",
      "NEXT STEPS:\n",
      "  1. Build Streamlit app for interactive optimization\n",
      "  2. Test with real pricing scenarios\n",
      "  3. A/B test recommendations vs current pricing\n",
      "  4. Monitor model performance in production\n",
      "  5. Retrain monthly with new data\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"11. SAVE FINAL MODEL FOR PRODUCTION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, 'price_optimization_model.pkl')\n",
    "print(f\"‚úì Saved: price_optimization_model.pkl ({best_model_name})\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "print(\"‚úì Saved: feature_scaler.pkl\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'features': all_features,\n",
    "    'target': target,\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'test_r2': float(best_model_r2),\n",
    "    'test_mae': float(test_mae_gb),\n",
    "    'test_rmse': float(test_rmse_gb),\n",
    "    'cv_mean_r2': float(cv_scores.mean()),\n",
    "    'cv_std_r2': float(cv_scores.std())\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(\"‚úì Saved: model_metadata.json\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL BUILDING COMPLETE - PRODUCTION READY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"DELIVERABLES:\")\n",
    "print(\"  ‚úì Trained model: price_optimization_model.pkl\")\n",
    "print(\"  ‚úì Feature scaler: feature_scaler.pkl\")\n",
    "print(\"  ‚úì Model metadata: model_metadata.json\")\n",
    "print(\"  ‚úì Feature definitions: feature_metadata.json\")\n",
    "print()\n",
    "\n",
    "print(\"MODEL PERFORMANCE:\")\n",
    "print(f\"  Test R¬≤: {best_model_r2:.4f} ({best_model_r2*100:.1f}% variance explained)\")\n",
    "print(f\"  Test MAE: ${test_mae_gb:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_gb:.1%}\")\n",
    "print()\n",
    "\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"  1. Build Streamlit app for interactive optimization\")\n",
    "print(\"  2. Test with real pricing scenarios\")\n",
    "print(\"  3. A/B test recommendations vs current pricing\")\n",
    "print(\"  4. Monitor model performance in production\")\n",
    "print(\"  5. Retrain monthly with new data\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
