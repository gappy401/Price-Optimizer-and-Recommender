{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1b6d4d",
   "metadata": {},
   "source": [
    "MODEL BUILDING FOR PRICE OPTIMIZATION\n",
    "\n",
    "BUSINESS OBJECTIVE: Build ML model to predict profit at different price points\n",
    "TECHNICAL OBJECTIVE: Compare models, select best for production deployment\n",
    "\n",
    "MODEL SELECTION CRITERIA:\n",
    "1. Prediction accuracy (R¬≤, MAE, RMSE)\n",
    "2. Interpretability (can we explain to stakeholders?)\n",
    "3. Speed (real-time pricing decisions)\n",
    "4. Robustness (works across all products/segments)\n",
    "\n",
    "MODELS TO TEST:\n",
    "1. Linear Regression - Baseline, highly interpretable\n",
    "2. Ridge Regression - Regularized linear, prevents overfitting\n",
    "3. Random Forest - Non-linear, feature importance, robust\n",
    "4. Gradient Boosting - Best accuracy, industry standard\n",
    "5. XGBoost - Production-ready, fast, accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fc1b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. LOAD DATA\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded: 10,000 records with 44 columns\n",
      "‚úì Features: 27\n",
      "‚úì Target: profit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA AND FEATURES\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"1. LOAD DATA\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "df = pd.read_csv('lab_equipment_pricing_features.csv')\n",
    "print(f\"‚úì Loaded: {len(df):,} records with {df.shape[1]} columns\")\n",
    "\n",
    "# Load feature metadata\n",
    "with open('feature_metadata.json', 'r') as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "all_features = feature_metadata['all_features']\n",
    "target = feature_metadata['target']\n",
    "\n",
    "print(f\"‚úì Features: {len(all_features)}\")\n",
    "print(f\"‚úì Target: {target}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc14611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "2. TRAIN/TEST SPLIT STRATEGY\n",
      "================================================================================\n",
      "\n",
      "WHY TIME-BASED SPLIT:\n",
      "  - Can't use random split (data leakage risk)\n",
      "  - Must simulate real scenario: train on past, predict future\n",
      "  - Business: Model must work on upcoming quarters\n",
      "\n",
      "Rows after removing NaN: 10,000\n",
      "\n",
      "Train set: 8,000 records (80%)\n",
      "Test set: 2,000 records (20%)\n",
      "Train target mean: $467,910\n",
      "Test target mean: $452,682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPARE TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"2. TRAIN/TEST SPLIT STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY TIME-BASED SPLIT:\")\n",
    "print(\"  - Can't use random split (data leakage risk)\")\n",
    "print(\"  - Must simulate real scenario: train on past, predict future\")\n",
    "print(\"  - Business: Model must work on upcoming quarters\")\n",
    "print()\n",
    "\n",
    "# Sort by date\n",
    "df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Remove rows with NaN in features (from rolling calculations)\n",
    "df_model = df_sorted[all_features + [target]].dropna()\n",
    "print(f\"Rows after removing NaN: {len(df_model):,}\")\n",
    "\n",
    "X = df_model[all_features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Time-based split: 80% train, 20% test\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} records (80%)\")\n",
    "print(f\"Test set: {len(X_test):,} records (20%)\")\n",
    "print(f\"Train target mean: ${y_train.mean():,.0f}\")\n",
    "print(f\"Test target mean: ${y_test.mean():,.0f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5627456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "3. FEATURE SCALING\n",
      "================================================================================\n",
      "\n",
      "WHY SCALING:\n",
      "  - Linear models sensitive to feature scales\n",
      "  - Tree models don't need scaling (but doesn't hurt)\n",
      "  - Speeds up convergence\n",
      "\n",
      "‚úì Features scaled using StandardScaler\n",
      "  Mean: ~0, Std: ~1 for all features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"3. FEATURE SCALING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY SCALING:\")\n",
    "print(\"  - Linear models sensitive to feature scales\")\n",
    "print(\"  - Tree models don't need scaling (but doesn't hurt)\")\n",
    "print(\"  - Speeds up convergence\")\n",
    "print()\n",
    "\n",
    "# 2. Identify Columns\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "# 3. Define Preprocessors\n",
    "# Transformation for numerical data (scaling)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Transformation for categorical data (encoding)\n",
    "# handle_unknown='ignore' prevents an error if a new category appears in test data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply the transformations\n",
    "X_train_scaled  = preprocessor.fit_transform(X_train)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "all_features_expanded = preprocessor.get_feature_names_out()\n",
    "\n",
    "print(\"‚úì Features scaled using StandardScaler\")\n",
    "print(f\"  Mean: ~0, Std: ~1 for all features\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e35855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4. MODEL 1: LINEAR REGRESSION (BASELINE)\n",
      "================================================================================\n",
      "\n",
      "WHY LINEAR REGRESSION:\n",
      "  Business: Highly interpretable, shows feature importance\n",
      "  Technical: Fast, simple, good baseline\n",
      "  Limitation: Assumes linear relationships\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.8636\n",
      "  Test R¬≤: 0.8532\n",
      "  Test MAE: $131,590\n",
      "  Test RMSE: $171,931\n",
      "  Test MAPE: 275.3%\n",
      "\n",
      "Top 10 Most Important Features (by coefficient magnitude):\n",
      "                feature    coefficient\n",
      "   num__inventory_level -617285.352717\n",
      "     num__inventory_pct  611923.355536\n",
      "       num__price_ma_7d  253838.593420\n",
      "      num__price_ma_30d  131500.702226\n",
      "   num__segment_encoded   99820.713118\n",
      "    num__price_momentum   35110.044630\n",
      " num__is_academic_start   34409.616418\n",
      "num__is_summer_slowdown  -33408.598185\n",
      "         num__qty_ma_7d   15778.984799\n",
      "     cat__season_Summer  -13839.388305\n",
      "\n",
      "BUSINESS INTERPRETATION:\n",
      "  Most influential: num__inventory_level (coef: -617285.35)\n",
      "  ‚Üí Each unit increase in num__inventory_level changes profit by $-617285.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: LINEAR REGRESSION (BASELINE)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"4. MODEL 1: LINEAR REGRESSION (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY LINEAR REGRESSION:\")\n",
    "print(\"  Business: Highly interpretable, shows feature importance\")\n",
    "print(\"  Technical: Fast, simple, good baseline\")\n",
    "print(\"  Limitation: Assumes linear relationships\")\n",
    "print()\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_lr = lr_model.predict(X_train_scaled)\n",
    "y_pred_test_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_r2_lr = r2_score(y_train, y_pred_train_lr)\n",
    "test_r2_lr = r2_score(y_test, y_pred_test_lr)\n",
    "test_mae_lr = mean_absolute_error(y_test, y_pred_test_lr)\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_test_lr))\n",
    "test_mape_lr = mean_absolute_percentage_error(y_test, y_pred_test_lr)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_lr:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_lr:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_lr:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_lr:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_lr:.1%}\")\n",
    "print()\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': all_features_expanded,\n",
    "    'coefficient': lr_model.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by coefficient magnitude):\")\n",
    "print(feature_importance_lr.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"BUSINESS INTERPRETATION:\")\n",
    "top_feature = feature_importance_lr.iloc[0]\n",
    "print(f\"  Most influential: {top_feature['feature']} (coef: {top_feature['coefficient']:.2f})\")\n",
    "print(f\"  ‚Üí Each unit increase in {top_feature['feature']} changes profit by ${top_feature['coefficient']:.2f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ac5e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "5. MODEL 2: RIDGE REGRESSION (REGULARIZED)\n",
      "================================================================================\n",
      "\n",
      "WHY RIDGE:\n",
      "  Business: Same interpretability as linear, but more robust\n",
      "  Technical: L2 regularization prevents overfitting\n",
      "  Use case: When we have many correlated features\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.8636\n",
      "  Test R¬≤: 0.8533\n",
      "  Test MAE: $131,553\n",
      "  Test RMSE: $171,869\n",
      "  Test MAPE: 274.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: RIDGE REGRESSION (REGULARIZED)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"5. MODEL 2: RIDGE REGRESSION (REGULARIZED)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY RIDGE:\")\n",
    "print(\"  Business: Same interpretability as linear, but more robust\")\n",
    "print(\"  Technical: L2 regularization prevents overfitting\")\n",
    "print(\"  Use case: When we have many correlated features\")\n",
    "print()\n",
    "\n",
    "ridge_model = Ridge(alpha=10.0)  # Regularization strength\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_ridge = ridge_model.predict(X_train_scaled)\n",
    "y_pred_test_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_r2_ridge = r2_score(y_train, y_pred_train_ridge)\n",
    "test_r2_ridge = r2_score(y_test, y_pred_test_ridge)\n",
    "test_mae_ridge = mean_absolute_error(y_test, y_pred_test_ridge)\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_test_ridge))\n",
    "test_mape_ridge = mean_absolute_percentage_error(y_test, y_pred_test_ridge)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_ridge:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_ridge:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_ridge:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_ridge:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_ridge:.1%}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964ecb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6. MODEL 3: RANDOM FOREST\n",
      "================================================================================\n",
      "\n",
      "WHY RANDOM FOREST:\n",
      "  Business: Captures non-linear relationships (price curves aren't straight lines)\n",
      "  Technical: Robust, handles outliers well, provides feature importance\n",
      "  Use case: When relationships are complex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 3: RANDOM FOREST (NON-LINEAR)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"6. MODEL 3: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY RANDOM FOREST:\")\n",
    "print(\"  Business: Captures non-linear relationships (price curves aren't straight lines)\")\n",
    "print(\"  Technical: Robust, handles outliers well, provides feature importance\")\n",
    "print(\"  Use case: When relationships are complex\")\n",
    "print()\n",
    "\n",
    "# 1. Define the numerical transformer to just \"pass through\" the data\n",
    "#    This explicitly tells the transformer to leave numerical columns alone.\n",
    "numeric_passthrough = 'passthrough'\n",
    "\n",
    "rf_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numerical features are passed through without scaling\n",
    "        ('num', numeric_passthrough, numerical_features),\n",
    "        # Categorical features are One-Hot Encoded\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    # Important: If you have other columns (like IDs) that aren't in these lists,\n",
    "    # remainder='passthrough' will include them as well.\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# Apply the transformation\n",
    "X_train_rfprocessed = rf_preprocessor.fit_transform(X_train)\n",
    "X_test_rfprocessed = rf_preprocessor.transform(X_test)\n",
    "all_rf_features=rf_preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fe64c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32600e+01,  5.00000e+00,  2.00000e+00, ...,  1.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       [ 9.02760e+02,  5.00000e+00,  2.00000e+00, ...,  1.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       [-1.16023e+03,  5.00000e+00,  2.00000e+00, ...,  1.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       ...,\n",
       "       [ 3.66600e+01,  1.20000e+01,  4.00000e+00, ...,  0.00000e+00,\n",
       "         0.00000e+00,  1.00000e+00],\n",
       "       [-7.37830e+02,  1.20000e+01,  4.00000e+00, ...,  0.00000e+00,\n",
       "         0.00000e+00,  1.00000e+00],\n",
       "       [ 9.66000e+00,  1.20000e+01,  4.00000e+00, ...,  0.00000e+00,\n",
       "         0.00000e+00,  1.00000e+00]], shape=(2000, 32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rfprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc12d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.9609\n",
      "  Test R¬≤: 0.9456\n",
      "  Test MAE: $61,488\n",
      "  Test RMSE: $104,623\n",
      "  Test MAPE: 17.4%\n",
      "\n",
      "Top 10 Most Important Features (by RF importance):\n",
      "                      feature  importance\n",
      "            num__price_ma_30d    0.283730\n",
      "             num__price_ma_7d    0.209109\n",
      "num__price_inventory_pressure    0.150832\n",
      "         num__product_encoded    0.143436\n",
      "         num__segment_encoded    0.077230\n",
      "   num__price_diff_competitor    0.039549\n",
      "               num__qty_ma_7d    0.023560\n",
      "              num__qty_ma_30d    0.010229\n",
      "      num__is_summer_slowdown    0.007854\n",
      "           cat__season_Summer    0.006949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,        # Number of trees\n",
    "    max_depth=15,            # Prevent overfitting\n",
    "    min_samples_split=20,    # Minimum samples to split\n",
    "    min_samples_leaf=10,     # Minimum samples per leaf\n",
    "    max_features='sqrt',     # Features per split\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(X_train_rfprocessed, y_train)  # Trees don't need scaling\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_rf = rf_model.predict(X_train_rfprocessed)\n",
    "y_pred_test_rf = rf_model.predict(X_test_rfprocessed)\n",
    "\n",
    "# Metrics\n",
    "train_r2_rf = r2_score(y_train, y_pred_train_rf)\n",
    "test_r2_rf = r2_score(y_test, y_pred_test_rf)\n",
    "test_mae_rf = mean_absolute_error(y_test, y_pred_test_rf)\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
    "test_mape_rf = mean_absolute_percentage_error(y_test, y_pred_test_rf)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_rf:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_rf:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_rf:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_rf:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_rf:.1%}\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': all_rf_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by RF importance):\")\n",
    "print(feature_importance_rf.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc642950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "7. MODEL 4: GRADIENT BOOSTING\n",
      "================================================================================\n",
      "\n",
      "WHY GRADIENT BOOSTING:\n",
      "  Business: Industry standard for pricing, high accuracy\n",
      "  Technical: Sequential learning, corrects previous errors\n",
      "  Use case: When we need best possible predictions\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Performance Metrics:\n",
      "  Train R¬≤: 0.9783\n",
      "  Test R¬≤: 0.9687\n",
      "  Test MAE: $49,395\n",
      "  Test RMSE: $79,347\n",
      "  Test MAPE: 18.2%\n",
      "\n",
      "Top 10 Most Important Features (by GB importance):\n",
      "                      feature  importance\n",
      "            num__price_ma_30d    0.309557\n",
      "         num__product_encoded    0.207839\n",
      "             num__price_ma_7d    0.182052\n",
      "         num__segment_encoded    0.108996\n",
      "num__price_inventory_pressure    0.078832\n",
      "   num__price_diff_competitor    0.026120\n",
      "      num__is_summer_slowdown    0.013983\n",
      "               num__qty_ma_7d    0.013827\n",
      "              num__qty_ma_30d    0.012595\n",
      "       num__is_academic_start    0.009419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 4: GRADIENT BOOSTING (BEST PERFORMANCE)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"7. MODEL 4: GRADIENT BOOSTING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY GRADIENT BOOSTING:\")\n",
    "print(\"  Business: Industry standard for pricing, high accuracy\")\n",
    "print(\"  Technical: Sequential learning, corrects previous errors\")\n",
    "print(\"  Use case: When we need best possible predictions\")\n",
    "print()\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,        # More trees = better but slower\n",
    "    learning_rate=0.05,      # Smaller = more conservative\n",
    "    max_depth=5,             # Tree depth\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    subsample=0.8,           # Use 80% of data per tree\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_model.fit(X_train_rfprocessed, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_gb = gb_model.predict(X_train_rfprocessed)\n",
    "y_pred_test_gb = gb_model.predict(X_test_rfprocessed)\n",
    "\n",
    "# Metrics\n",
    "train_r2_gb = r2_score(y_train, y_pred_train_gb)\n",
    "test_r2_gb = r2_score(y_test, y_pred_test_gb)\n",
    "test_mae_gb = mean_absolute_error(y_test, y_pred_test_gb)\n",
    "test_rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_test_gb))\n",
    "test_mape_gb = mean_absolute_percentage_error(y_test, y_pred_test_gb)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  Train R¬≤: {train_r2_gb:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2_gb:.4f}\")\n",
    "print(f\"  Test MAE: ${test_mae_gb:,.0f}\")\n",
    "print(f\"  Test RMSE: ${test_rmse_gb:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_gb:.1%}\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': all_rf_features,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by GB importance):\")\n",
    "print(feature_importance_gb.head(10).to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1c4147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "8. MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "            Model  Train_R¬≤  Test_R¬≤    Test_MAE   Test_RMSE  Test_MAPE_%\n",
      "Linear Regression    0.8636   0.8532 131590.0954 171930.7271     275.3054\n",
      " Ridge Regression    0.8636   0.8533 131553.1551 171869.0836     274.9159\n",
      "    Random Forest    0.9609   0.9456  61487.6324 104623.2447      17.3943\n",
      "Gradient Boosting    0.9783   0.9687  49394.7970  79346.6484      18.2402\n",
      "\n",
      "üèÜ BEST MODEL: Gradient Boosting\n",
      "   Test R¬≤: 0.9687\n",
      "   Explains 96.9% of profit variation\n",
      "\n",
      "BUSINESS INTERPRETATION:\n",
      "  ‚úì Gradient Boosting wins - complex non-linear relationships captured\n",
      "  ‚úì Trade-off: Less interpretable than linear models\n",
      "  ‚úì Solution: Use SHAP values or feature importance for explainability\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"8. MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Train_R¬≤': [train_r2_lr, train_r2_ridge, train_r2_rf, train_r2_gb],\n",
    "    'Test_R¬≤': [test_r2_lr, test_r2_ridge, test_r2_rf, test_r2_gb],\n",
    "    'Test_MAE': [test_mae_lr, test_mae_ridge, test_mae_rf, test_mae_gb],\n",
    "    'Test_RMSE': [test_rmse_lr, test_rmse_ridge, test_rmse_rf, test_rmse_gb],\n",
    "    'Test_MAPE_%': [test_mape_lr*100, test_mape_ridge*100, test_mape_rf*100, test_mape_gb*100]\n",
    "})\n",
    "\n",
    "comparison = comparison.round(4)\n",
    "print(comparison.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Select best model\n",
    "best_idx = comparison['Test_R¬≤'].idxmax()\n",
    "best_model_name = comparison.loc[best_idx, 'Model']\n",
    "best_model_r2 = comparison.loc[best_idx, 'Test_R¬≤']\n",
    "\n",
    "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {best_model_r2:.4f}\")\n",
    "print(f\"   Explains {best_model_r2*100:.1f}% of profit variation\")\n",
    "print()\n",
    "\n",
    "# Business interpretation\n",
    "print(\"BUSINESS INTERPRETATION:\")\n",
    "if best_model_name == 'Gradient Boosting':\n",
    "    print(\"  ‚úì Gradient Boosting wins - complex non-linear relationships captured\")\n",
    "    print(\"  ‚úì Trade-off: Less interpretable than linear models\")\n",
    "    print(\"  ‚úì Solution: Use SHAP values or feature importance for explainability\")\n",
    "    best_model = gb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    print(\"  ‚úì Random Forest wins - good balance of accuracy and speed\")\n",
    "    print(\"  ‚úì Feature importance built-in for explainability\")\n",
    "    best_model = rf_model\n",
    "else:\n",
    "    print(\"  ‚úì Linear model wins - simple relationships, highly interpretable\")\n",
    "    print(\"  ‚úì Can directly explain coefficient impact\")\n",
    "    best_model = ridge_model if best_model_name == 'Ridge Regression' else lr_model\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08784386",
   "metadata": {},
   "source": [
    " Since the Mean Absolute Error (MAE) was $\\mathbf{\\$49,395}$, the fact that the RMSE ($\\$79,300$) is higher tells us that the model has a number of larger, isolated errors (outliers) that pull the RMSE up significantly beyond the MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69d56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "9. CROSS-VALIDATION (TIME SERIES)\n",
      "================================================================================\n",
      "\n",
      "WHY CROSS-VALIDATION:\n",
      "  - Validate model stability across different time periods\n",
      "  - Detect overfitting\n",
      "  - Ensure model works on future data\n",
      "\n",
      "Running 5-fold time series cross-validation on best model...\n",
      "\n",
      "CV R¬≤ Scores: ['0.8583', '0.9571', '0.9638', '0.9625', '0.9652']\n",
      "Mean CV R¬≤: 0.9414 (+/- 0.0416)\n",
      "\n",
      "‚úì Low variance across folds - model is stable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"9. CROSS-VALIDATION (TIME SERIES)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY CROSS-VALIDATION:\")\n",
    "print(\"  - Validate model stability across different time periods\")\n",
    "print(\"  - Detect overfitting\")\n",
    "print(\"  - Ensure model works on future data\")\n",
    "print()\n",
    "\n",
    "# Time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Running 5-fold time series cross-validation on best model...\")\n",
    "cv_scores = cross_val_score(best_model, X_train_rfprocessed, y_train, cv=tscv, \n",
    "                             scoring='r2', n_jobs=-1)\n",
    "\n",
    "print(f\"\\nCV R¬≤ Scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "print(f\"Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "print()\n",
    "\n",
    "if cv_scores.std() < 0.05:\n",
    "    print(\"‚úì Low variance across folds - model is stable\")\n",
    "else:\n",
    "    print(\"‚ö† High variance - model performance varies by time period\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83efc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "10. RESIDUAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "WHY RESIDUALS:\n",
      "  - Check for systematic errors (bias)\n",
      "  - Validate model assumptions\n",
      "  - Identify segments where model struggles\n",
      "\n",
      "Residual Statistics:\n",
      "  Mean error: $-1,379 (should be ~0)\n",
      "  Median error: $-15\n",
      "  Std dev: $79,355\n",
      "\n",
      "Mean Absolute Error by Product:\n",
      "product\n",
      "Microscope     $97,722\n",
      "Centrifuge     $86,129\n",
      "PCR_System     $54,889\n",
      "Reagent_Kit     $6,422\n",
      "Pipettes        $6,271\n",
      "Name: abs_error, dtype: object\n",
      "\n",
      "BUSINESS INSIGHT:\n",
      "  Hardest to predict: Microscope (error: $97,722)\n",
      "  Easiest to predict: Pipettes (error: $6,271)\n",
      "  ‚Üí May need product-specific models or more features for Microscope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RESIDUAL ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"10. RESIDUAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"WHY RESIDUALS:\")\n",
    "print(\"  - Check for systematic errors (bias)\")\n",
    "print(\"  - Validate model assumptions\")\n",
    "print(\"  - Identify segments where model struggles\")\n",
    "print()\n",
    "\n",
    "residuals = y_test - y_pred_test_gb\n",
    "\n",
    "print(\"Residual Statistics:\")\n",
    "print(f\"  Mean error: ${residuals.mean():,.0f} (should be ~0)\")\n",
    "print(f\"  Median error: ${residuals.median():,.0f}\")\n",
    "print(f\"  Std dev: ${residuals.std():,.0f}\")\n",
    "print()\n",
    "\n",
    "# Check for bias by segment\n",
    "df_test = df_sorted[split_idx:split_idx+len(y_test)].copy()\n",
    "df_test['residual'] = residuals.values\n",
    "df_test['abs_error'] = np.abs(residuals.values)\n",
    "\n",
    "print(\"Mean Absolute Error by Product:\")\n",
    "product_errors = df_test.groupby('product')['abs_error'].mean().sort_values(ascending=False)\n",
    "print(product_errors.apply(lambda x: f\"${x:,.0f}\"))\n",
    "print()\n",
    "\n",
    "print(\"BUSINESS INSIGHT:\")\n",
    "worst_product = product_errors.index[0]\n",
    "best_product = product_errors.index[-1]\n",
    "print(f\"  Hardest to predict: {worst_product} (error: ${product_errors.iloc[0]:,.0f})\")\n",
    "print(f\"  Easiest to predict: {best_product} (error: ${product_errors.iloc[-1]:,.0f})\")\n",
    "print(f\"  ‚Üí May need product-specific models or more features for {worst_product}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba3cf1",
   "metadata": {},
   "source": [
    "The key takeaway is that your model is accurate overall, but it reveals massive pricing volatility and risk in certain product segments.\n",
    "\n",
    "1. Overall Confidence and RiskMean Error ($\\$-1,379$): This means that on average, our model's predictions for the final profit are unbiased. We are not systematically over- or under-forecasting profit across the entire business. \n",
    "\n",
    "    Stakeholder takeaway: The model is fair and trustworthy.RMSE / Standard Deviation ($\\approx \\$79,300$): This represents our typical error or risk margin for any given deal.\n",
    "\n",
    "    Interpretation: For a new, typical sale, we can expect our profit forecast to be off by $\\mathbf{\\$79,300}$ in either direction (up or down).\n",
    "\n",
    "    Pricing Relevance: This is the size of the uncertainty buffer you should factor into large quotes or budgeting. Since the standard deviation ($\\$79,300$) is much higher than the average error (MAE of $\\approx \\$49,400$), it confirms that a few deals have exceptionally high volatility, which leads us to the next point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a207f",
   "metadata": {},
   "source": [
    "2. Pinpointing Pricing Volatility (The Product Breakdown) The breakdown of error by product is the most actionable part of this analysis for pricing. It tells you which products have consistent pricing and which have chaotic or highly customized pricing.\n",
    "\n",
    "    A. Microscope and Centrifuge hasve MAE of around $\\mathbf{\\$90,000}$ signifing High Volatility & Pricing Chaos. The error is double the overall model's average error. This suggests sales or pricing for these products are highly inconsistent, heavily customized, or influenced by non-modeled factors (like deep discounts, special bundles, or competitor pricing not in your data).\n",
    "    \n",
    "        Action: This product requires a standardized pricing matrix or better feature tracking.\n",
    "\n",
    "    B. Pipettes and Reagent Kits have MAE of aorund $\\mathbf{\\$6,300}$ signifing pricing Predictability & Consistency. The error is very low, meaning the factors driving Pipette  and Reagent kits profit are clear, consistent, and well-captured by the model. \n",
    "    \n",
    "        Action: Trust the model's forecasts and pricing recommendations for these product lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "096dfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "11. SAVE FINAL MODEL FOR PRODUCTION\n",
      "================================================================================\n",
      "\n",
      "‚úì Saved: price_optimization_model.pkl (Gradient Boosting)\n",
      "‚úì Saved: feature_scaler.pkl\n",
      "‚úì Saved: feature_scaler_rf.pkl\n",
      "‚úì Saved: model_metadata.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"11. SAVE FINAL MODEL FOR PRODUCTION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, 'price_optimization_model.pkl')\n",
    "print(f\"‚úì Saved: price_optimization_model.pkl ({best_model_name})\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(preprocessor, 'feature_scaler_1.pkl')\n",
    "print(\"‚úì Saved: feature_scaler.pkl\")\n",
    "\n",
    "joblib.dump(rf_preprocessor, 'feature_scaler_rf.pkl')\n",
    "print(\"‚úì Saved: feature_scaler_rf.pkl\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'features': all_rf_features.tolist(),\n",
    "    'features_expanded': all_features_expanded.tolist(),\n",
    "    'target': target,\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'test_r2': float(best_model_r2),\n",
    "    'test_mae': float(test_mae_gb),\n",
    "    'test_rmse': float(test_rmse_gb),\n",
    "    'cv_mean_r2': float(cv_scores.mean()),\n",
    "    'cv_std_r2': float(cv_scores.std())\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(\"‚úì Saved: model_metadata.json\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc917d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL BUILDING COMPLETE - PRODUCTION READY\n",
      "================================================================================\n",
      "\n",
      "DELIVERABLES:\n",
      "  ‚úì Trained model: price_optimization_model.pkl\n",
      "  ‚úì Feature scaler: feature_scaler.pkl\n",
      "  ‚úì Model metadata: model_metadata.json\n",
      "  ‚úì Feature definitions: feature_metadata.json\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "  Test R¬≤: 0.9687 (96.9% variance explained)\n",
      "  Test MAE: $49,395\n",
      "  Test MAPE: 18.2%\n",
      "\n",
      "NEXT STEPS:\n",
      "  1. Build Streamlit app for interactive optimization\n",
      "  2. Test with real pricing scenarios\n",
      "  3. A/B test recommendations vs current pricing\n",
      "  4. Monitor model performance in production\n",
      "  5. Retrain monthly with new data\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL BUILDING COMPLETE - PRODUCTION READY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"DELIVERABLES:\")\n",
    "print(\"  ‚úì Trained model: price_optimization_model.pkl\")\n",
    "print(\"  ‚úì Feature scaler: feature_scaler.pkl\")\n",
    "print(\"  ‚úì Model metadata: model_metadata.json\")\n",
    "print(\"  ‚úì Feature definitions: feature_metadata.json\")\n",
    "print()\n",
    "\n",
    "print(\"MODEL PERFORMANCE:\")\n",
    "print(f\"  Test R¬≤: {best_model_r2:.4f} ({best_model_r2*100:.1f}% variance explained)\")\n",
    "print(f\"  Test MAE: ${test_mae_gb:,.0f}\")\n",
    "print(f\"  Test MAPE: {test_mape_gb:.1%}\")\n",
    "print()\n",
    "\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"  1. Build Streamlit app for interactive optimization\")\n",
    "print(\"  2. Test with real pricing scenarios\")\n",
    "print(\"  3. A/B test recommendations vs current pricing\")\n",
    "print(\"  4. Monitor model performance in production\")\n",
    "print(\"  5. Retrain monthly with new data\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
